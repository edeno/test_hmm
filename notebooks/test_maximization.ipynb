{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jax import lax\n",
    "from jax import vmap\n",
    "from jax import jit\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def _normalize(u, axis=0, eps=1e-15):\n",
    "    \"\"\"Normalizes the values within the axis in a way that they sum up to 1.\n",
    "    Args:\n",
    "        u: Input array to normalize.\n",
    "        axis: Axis over which to normalize.\n",
    "        eps: Minimum value threshold for numerical stability.\n",
    "    Returns:\n",
    "        Tuple of the normalized values, and the normalizing denominator.\n",
    "    \"\"\"\n",
    "    u = jnp.where(u == 0, 0, jnp.where(u < eps, eps, u))\n",
    "    c = u.sum(axis=axis)\n",
    "    c = jnp.where(c == 0, 1, c)\n",
    "    return u / c, c\n",
    "\n",
    "\n",
    "# Helper functions for the two key filtering steps\n",
    "def _condition_on(probs, ll):\n",
    "    \"\"\"Condition on new emissions, given in the form of log likelihoods\n",
    "    for each discrete state, while avoiding numerical underflow.\n",
    "    Args:\n",
    "        probs(k): prior for state k\n",
    "        ll(k): log likelihood for state k\n",
    "    Returns:\n",
    "        probs(k): posterior for state k\n",
    "    \"\"\"\n",
    "    ll_max = ll.max()\n",
    "    new_probs = probs * jnp.exp(ll - ll_max)\n",
    "    new_probs, norm = _normalize(new_probs)\n",
    "    log_norm = jnp.log(norm) + ll_max\n",
    "    return new_probs, log_norm\n",
    "\n",
    "\n",
    "def _predict(probs, A):\n",
    "    return A.T @ probs\n",
    "\n",
    "\n",
    "@partial(jit)\n",
    "def hmm_filter(\n",
    "    initial_distribution,\n",
    "    transition_matrix,\n",
    "    log_likelihoods,\n",
    "):\n",
    "    r\"\"\"Forwards filtering\n",
    "    Transition matrix may be either 2D (if transition probabilities are fixed) or 3D\n",
    "    if the transition probabilities vary over time. Alternatively, the transition\n",
    "    matrix may be specified via `transition_fn`, which takes in a time index $t$ and\n",
    "    returns a transition matrix.\n",
    "    Args:\n",
    "        initial_distribution: $p(z_1 \\mid u_1, \\theta)$\n",
    "        transition_matrix: $p(z_{t+1} \\mid z_t, u_t, \\theta)$\n",
    "        log_likelihoods: $p(y_t \\mid z_t, u_t, \\theta)$ for $t=1,\\ldots, T$.\n",
    "    Returns:\n",
    "        filtered posterior distribution\n",
    "    \"\"\"\n",
    "    num_timesteps, num_states = log_likelihoods.shape\n",
    "\n",
    "    def _step(carry, t):\n",
    "        log_normalizer, predicted_probs = carry\n",
    "\n",
    "        ll = log_likelihoods[t]\n",
    "\n",
    "        filtered_probs, log_norm = _condition_on(predicted_probs, ll)\n",
    "        log_normalizer += log_norm\n",
    "        predicted_probs_next = _predict(filtered_probs, transition_matrix)\n",
    "\n",
    "        return (log_normalizer, predicted_probs_next), (filtered_probs, predicted_probs)\n",
    "\n",
    "    carry = (0.0, initial_distribution)\n",
    "    (log_normalizer, _), (filtered_probs, predicted_probs) = lax.scan(\n",
    "        _step, carry, jnp.arange(num_timesteps)\n",
    "    )\n",
    "\n",
    "    return log_normalizer, filtered_probs, predicted_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-1.1920929e-07, dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "initial_distribution = np.asarray([0.9, 0.1])\n",
    "transition_matrix = np.asarray([[0.9, 0.1], [0.1, 0.9]])\n",
    "log_likelihoods = np.zeros((9, 2))\n",
    "\n",
    "marginal_likelihood, _, _ = hmm_filter(\n",
    "    initial_distribution,\n",
    "    transition_matrix,\n",
    "    log_likelihoods,\n",
    ")\n",
    "marginal_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.nn import softmax\n",
    "\n",
    "\n",
    "def centered_softmax_forward(y):\n",
    "    \"\"\"`softmax(x) = exp(x-c) / sum(exp(x-c))` where c is the last coordinate\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    > y = np.log([2, 3, 4])\n",
    "    > np.allclose(centered_softmax_forward(y), [0.2, 0.3, 0.4, 0.1])\n",
    "    \"\"\"\n",
    "    if y.ndim == 1:\n",
    "        y = jnp.append(y, 0)\n",
    "    else:\n",
    "        y = jnp.column_stack((y, jnp.zeros((y.shape[0],))))\n",
    "\n",
    "    return softmax(y, axis=-1)\n",
    "\n",
    "\n",
    "def centered_softmax_inverse(y):\n",
    "    \"\"\"`softmax(x) = exp(x-c) / sum(exp(x-c))` where c is the last coordinate\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    > y = np.asarray([0.2, 0.3, 0.4, 0.1])\n",
    "    > np.allclose(np.exp(centered_softmax_inverse(y)), np.asarray([2,3,4]))\n",
    "    \"\"\"\n",
    "    return jnp.log(y[..., :-1]) - jnp.log(y[..., [-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import jax\n",
    "from src.simulate import simulate_two_state_poisson\n",
    "\n",
    "\n",
    "sampling_frequency = 500\n",
    "time, true_rate, spikes = simulate_two_state_poisson(\n",
    "    sampling_frequency=sampling_frequency\n",
    ")\n",
    "\n",
    "n_states = 2\n",
    "n_time = spikes.shape[0]\n",
    "initial_distribution = np.ones((n_states,)) / n_states\n",
    "transition_matrix = np.asarray([[0.98, 0.02], [0.02, 0.98]])\n",
    "is_training = np.ones((n_time,), dtype=bool)\n",
    "is_training[: n_time // 2] = False\n",
    "\n",
    "n_rate_parameters = 1\n",
    "n_rates = n_states * n_rate_parameters\n",
    "\n",
    "\n",
    "def transform_parameters(log_parameters):\n",
    "    unconstrained_rates = log_parameters[:n_rates]\n",
    "    unconstrained_initial_distribution = log_parameters[\n",
    "        n_rates : n_rates + n_states - 1\n",
    "    ]\n",
    "    unconstrained_transition_matrix = log_parameters[n_rates + n_states - 1 :].reshape((n_states, n_states - 1))\n",
    "\n",
    "    rates = jnp.exp(unconstrained_rates)\n",
    "    initial_distribution = centered_softmax_forward(unconstrained_initial_distribution)\n",
    "    transition_matrix = centered_softmax_forward(unconstrained_transition_matrix)\n",
    "\n",
    "    return rates, initial_distribution, transition_matrix\n",
    "\n",
    "@jax.jit\n",
    "def neglogp(log_parameters):\n",
    "    rates, initial_distribution, transition_matrix = transform_parameters(log_parameters)\n",
    "\n",
    "    observation_log_likelihood = jax.scipy.stats.poisson.logpmf(spikes[:, jnp.newaxis], rates)\n",
    "\n",
    "    marginal_log_likelihood, _, _ = hmm_filter(\n",
    "        initial_distribution, transition_matrix, observation_log_likelihood\n",
    "    )\n",
    "\n",
    "    return -1.0 * marginal_log_likelihood\n",
    "\n",
    "\n",
    "dlike = jax.grad(neglogp)\n",
    "\n",
    "x0 = np.concatenate(\n",
    "    (\n",
    "        np.log([spikes[is_training].mean(), spikes[~is_training].mean()]), # 2\n",
    "        centered_softmax_inverse(initial_distribution), # n_states - 1\n",
    "        centered_softmax_inverse(transition_matrix).ravel(), # n_states * (n_states - 1)\n",
    "    )\n",
    ")\n",
    "\n",
    "res = minimize(\n",
    "    neglogp,\n",
    "    x0=x0,\n",
    "    method=\"BFGS\",\n",
    "    jac=dlike,\n",
    ")\n",
    "\n",
    "estimated_rates, estimated_initial_distribution, estimated_transition_matrix = transform_parameters(res.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.0144015 , 0.98559844], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_initial_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[9.9977672e-01, 2.2317682e-04],\n",
       "       [3.0101385e-04, 9.9969894e-01]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([19.987234 ,  4.9286847], dtype=float32), array([ 5., 20.]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_rates * sampling_frequency, np.unique(true_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_hmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6c357aac43d0ff46917fb823d0877c209998debada17a7fc46c62b2ec31f1f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
