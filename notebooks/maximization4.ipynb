{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax.nn import softmax\n",
    "from scipy.optimize import minimize\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def _normalize(u, axis=0, eps=1e-15):\n",
    "    \"\"\"Normalizes the values within the axis in a way that they sum up to 1.\n",
    "    Args:\n",
    "        u: Input array to normalize.\n",
    "        axis: Axis over which to normalize.\n",
    "        eps: Minimum value threshold for numerical stability.\n",
    "    Returns:\n",
    "        Tuple of the normalized values, and the normalizing denominator.\n",
    "    \"\"\"\n",
    "    u = jnp.where(u == 0, 0, jnp.where(u < eps, eps, u))\n",
    "    c = u.sum(axis=axis)\n",
    "    c = jnp.where(c == 0, 1, c)\n",
    "    return u / c, c\n",
    "\n",
    "\n",
    "# Helper functions for the two key filtering steps\n",
    "def _condition_on(probs, ll):\n",
    "    \"\"\"Condition on new emissions, given in the form of log likelihoods\n",
    "    for each discrete state, while avoiding numerical underflow.\n",
    "    Args:\n",
    "        probs(k): prior for state k\n",
    "        ll(k): log likelihood for state k\n",
    "    Returns:\n",
    "        probs(k): posterior for state k\n",
    "    \"\"\"\n",
    "    ll_max = ll.max()\n",
    "    new_probs = probs * jnp.exp(ll - ll_max)\n",
    "    new_probs, norm = _normalize(new_probs)\n",
    "    log_norm = jnp.log(norm) + ll_max\n",
    "    return new_probs, log_norm\n",
    "\n",
    "\n",
    "def _predict(probs, A):\n",
    "    return A.T @ probs\n",
    "\n",
    "def _get_transition_matrix(transition_matrix, t):\n",
    "        return transition_matrix[t] if transition_matrix.ndim == 3 else transition_matrix\n",
    "\n",
    "\n",
    "@partial(jax.jit)\n",
    "def hmm_filter(\n",
    "    initial_distribution,\n",
    "    transition_matrix,\n",
    "    log_likelihoods,\n",
    "):\n",
    "    r\"\"\"Forwards filtering\n",
    "    Transition matrix may be either 2D (if transition probabilities are fixed) or 3D\n",
    "    if the transition probabilities vary over time. Alternatively, the transition\n",
    "    matrix may be specified via `transition_fn`, which takes in a time index $t$ and\n",
    "    returns a transition matrix.\n",
    "    Args:\n",
    "        initial_distribution: $p(z_1 \\mid u_1, \\theta)$\n",
    "        transition_matrix: $p(z_{t+1} \\mid z_t, u_t, \\theta)$\n",
    "        log_likelihoods: $p(y_t \\mid z_t, u_t, \\theta)$ for $t=1,\\ldots, T$.\n",
    "    Returns:\n",
    "        filtered posterior distribution\n",
    "    \"\"\"\n",
    "    num_timesteps, num_states = log_likelihoods.shape\n",
    "\n",
    "    def _step(carry, t):\n",
    "        log_normalizer, predicted_probs = carry\n",
    "\n",
    "        ll = log_likelihoods[t]\n",
    "\n",
    "        filtered_probs, log_norm = _condition_on(predicted_probs, ll)\n",
    "        log_normalizer += log_norm\n",
    "        predicted_probs_next = _predict(filtered_probs, transition_matrix)\n",
    "\n",
    "        return (log_normalizer, predicted_probs_next), (filtered_probs, predicted_probs)\n",
    "\n",
    "    carry = (0.0, initial_distribution)\n",
    "    (log_normalizer, _), (filtered_probs, predicted_probs) = jax.lax.scan(\n",
    "        _step, carry, jnp.arange(num_timesteps)\n",
    "    )\n",
    "\n",
    "    return log_normalizer, filtered_probs, predicted_probs\n",
    "\n",
    "\n",
    "def fit_regression(design_matrix, weights, spikes):\n",
    "    @jax.jit\n",
    "    def neglogp(\n",
    "        coefficients, spikes=spikes, design_matrix=design_matrix, weights=weights\n",
    "    ):\n",
    "        conditional_intensity = jnp.exp(design_matrix @ coefficients)\n",
    "        conditional_intensity = jnp.clip(conditional_intensity, a_min=1e-15, a_max=None)\n",
    "        log_likelihood = weights * jax.scipy.stats.poisson.logpmf(\n",
    "            spikes, conditional_intensity\n",
    "        )\n",
    "        return -log_likelihood.sum()\n",
    "\n",
    "    dlike = jax.grad(neglogp)\n",
    "\n",
    "    initial_condition = np.array([np.log(np.average(spikes, weights=weights))])\n",
    "    initial_condition = np.concatenate(\n",
    "        [initial_condition, np.zeros(design_matrix.shape[1] - 1)]\n",
    "    )\n",
    "\n",
    "    res = minimize(neglogp, x0=initial_condition, method=\"BFGS\", jac=dlike)\n",
    "\n",
    "    return res.x\n",
    "\n",
    "\n",
    "def centered_softmax_forward(y):\n",
    "    \"\"\"`softmax(x) = exp(x-c) / sum(exp(x-c))` where c is the last coordinate\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    > y = np.log([2, 3, 4])\n",
    "    > np.allclose(centered_softmax_forward(y), [0.2, 0.3, 0.4, 0.1])\n",
    "    \"\"\"\n",
    "    if y.ndim == 1:\n",
    "        y = jnp.append(y, 0)\n",
    "    else:\n",
    "        y = jnp.column_stack((y, jnp.zeros((y.shape[0],))))\n",
    "\n",
    "    return softmax(y, axis=-1)\n",
    "\n",
    "\n",
    "def centered_softmax_inverse(y):\n",
    "    \"\"\"`softmax(x) = exp(x-c) / sum(exp(x-c))` where c is the last coordinate\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    > y = np.asarray([0.2, 0.3, 0.4, 0.1])\n",
    "    > np.allclose(np.exp(centered_softmax_inverse(y)), np.asarray([2,3,4]))\n",
    "    \"\"\"\n",
    "    return jnp.log(y[..., :-1]) - jnp.log(y[..., [-1]])\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def neglogp(\n",
    "    unconstrained_parameters,\n",
    "    initial_distribution,\n",
    "    observation_log_likelihood,\n",
    "):\n",
    "\n",
    "    # Unpack parameters\n",
    "    # initial_distribution = centered_softmax_forward(unconstrained_parameters)\n",
    "    n_states = initial_distribution.shape[0]\n",
    "    transition_matrix = centered_softmax_forward(unconstrained_parameters.reshape((n_states, n_states - 1)))\n",
    "\n",
    "    marginal_log_likelihood, _, _ = hmm_filter(\n",
    "        initial_distribution, transition_matrix, observation_log_likelihood\n",
    "    )\n",
    "    nll = -1.0 * marginal_log_likelihood\n",
    "    jax.debug.print(\"neg. log like.: {}\", nll)\n",
    "\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn import hmm\n",
    "\n",
    "\n",
    "n_states = 3\n",
    "sampling_frequency = 500.0\n",
    "n_time = 100_000\n",
    "\n",
    "model1 = hmm.PoissonHMM(n_components=n_states)\n",
    "\n",
    "model1.startprob_ = np.array([0.10, 0.10, 0.80])\n",
    "\n",
    "model1.transmat_ = np.array(\n",
    "    [[0.99, 0.005, 0.005],\n",
    "     [0.005, 0.99, 0.005],\n",
    "     [0.005, 0.005, 0.99]]\n",
    ")\n",
    "\n",
    "model1.lambdas_ = np.array([10.0, 25.0, 50.0])[:, np.newaxis] / sampling_frequency\n",
    "\n",
    "spikes1, state_sequence1 = model1.sample(n_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_distribution = np.ones((n_states,)) / n_states\n",
    "initial_distribution = model1.startprob_\n",
    "\n",
    "transition_matrix = np.array(\n",
    "    [[0.90, 0.01, 0.01],\n",
    "     [0.01, 0.90, 0.01],\n",
    "     [0.01, 0.01, 0.90]]\n",
    ")\n",
    "local_rates = model1.lambdas_.squeeze()\n",
    "\n",
    "x0 = centered_softmax_inverse(transition_matrix).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg. log like.: 20953.4609375\n",
      "neg. log like.: 20953.4609375\n",
      "neg. log like.: 20962.16796875\n",
      "neg. log like.: 20962.16796875\n",
      "neg. log like.: 20930.037109375\n",
      "neg. log like.: 20930.037109375\n",
      "neg. log like.: 20920.26953125\n",
      "neg. log like.: 20920.26953125\n",
      "neg. log like.: 20914.458984375\n",
      "neg. log like.: 20914.458984375\n",
      "neg. log like.: 20912.802734375\n",
      "neg. log like.: 20912.802734375\n",
      "neg. log like.: 20942.30078125\n",
      "neg. log like.: 20942.30078125\n",
      "neg. log like.: 20910.52734375\n",
      "neg. log like.: 20910.52734375\n",
      "neg. log like.: 20915.470703125\n",
      "neg. log like.: 20915.470703125\n",
      "neg. log like.: 20910.072265625\n",
      "neg. log like.: 20910.072265625\n",
      "neg. log like.: 20909.388671875\n",
      "neg. log like.: 20909.388671875\n",
      "neg. log like.: 20909.22265625\n",
      "neg. log like.: 20909.22265625\n",
      "neg. log like.: 20908.787109375\n",
      "neg. log like.: 20908.787109375\n",
      "neg. log like.: 20909.46875\n",
      "neg. log like.: 20909.46875\n",
      "neg. log like.: 20908.705078125\n",
      "neg. log like.: 20908.705078125\n",
      "neg. log like.: 20908.759765625\n",
      "neg. log like.: 20908.759765625\n",
      "neg. log like.: 20908.685546875\n",
      "neg. log like.: 20908.685546875\n",
      "neg. log like.: 20908.638671875\n",
      "neg. log like.: 20908.638671875\n",
      "neg. log like.: 20908.568359375\n",
      "neg. log like.: 20908.568359375\n",
      "neg. log like.: 20908.443359375\n",
      "neg. log like.: 20908.443359375\n",
      "neg. log like.: 20908.318359375\n",
      "neg. log like.: 20908.318359375\n",
      "neg. log like.: 20908.369140625\n",
      "neg. log like.: 20908.369140625\n",
      "neg. log like.: 20908.310546875\n",
      "neg. log like.: 20908.310546875\n",
      "neg. log like.: 20908.30078125\n",
      "neg. log like.: 20908.30078125\n",
      "neg. log like.: 20908.333984375\n",
      "neg. log like.: 20908.333984375\n",
      "neg. log like.: 20908.30078125\n",
      "neg. log like.: 20908.30078125\n",
      "neg. log like.: 20908.294921875\n",
      "neg. log like.: 20908.294921875\n",
      "neg. log like.: 20908.31640625\n",
      "neg. log like.: 20908.31640625\n",
      "neg. log like.: 20908.32421875\n",
      "neg. log like.: 20908.32421875\n",
      "neg. log like.: 20908.294921875\n",
      "neg. log like.: 20908.294921875\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.291015625\n",
      "neg. log like.: 20908.31640625\n",
      "neg. log like.: 20908.314453125\n",
      "neg. log like.: 20908.31640625\n",
      "neg. log like.: 20908.30859375\n",
      "neg. log like.: 20908.302734375\n",
      "neg. log like.: 20908.30859375\n",
      "neg. log like.: 20908.314453125\n",
      "neg. log like.: 20908.3046875\n",
      "neg. log like.: 20908.296875\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.29296875\n",
      "neg. log like.: 20908.29296875\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 20908.294922\n",
      "         Iterations: 14\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00318886, -0.00355115,  0.00036236],\n",
       "       [-0.00177907,  0.00263436, -0.00085529],\n",
       "       [ 0.00028294,  0.00041303, -0.00069593]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.optimize\n",
    "\n",
    "observation_log_likelihood = scipy.stats.poisson.logpmf(spikes1, local_rates[np.newaxis])\n",
    "\n",
    "res = scipy.optimize.minimize(\n",
    "    neglogp,\n",
    "    x0=x0,\n",
    "    method=\"BFGS\",\n",
    "    jac=jax.grad(neglogp),\n",
    "    args=(\n",
    "        initial_distribution,\n",
    "        observation_log_likelihood,\n",
    "    ),\n",
    "    options={\n",
    "        \"disp\": True,\n",
    "        'gtol': 1e-2,  # Adjust gradient tolerance value\n",
    "        \n",
    "    },    \n",
    ")\n",
    "\n",
    "np.array(centered_softmax_forward(res.x.reshape((n_states, n_states - 1)))) - model1.transmat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 20908.294921875\n",
       " hess_inv: array([[ 0.03045055,  0.09985725, -0.00478979, -0.01431632, -0.01596873,\n",
       "         0.01098402],\n",
       "       [ 0.09985725,  0.71826047,  0.19469098, -0.07738144, -0.07425328,\n",
       "         0.09445286],\n",
       "       [-0.00478979,  0.19469098,  0.29474507,  0.12904511, -0.05047152,\n",
       "        -0.0399247 ],\n",
       "       [-0.01431632, -0.07738144,  0.12904511,  0.19918681,  0.01372938,\n",
       "        -0.13622989],\n",
       "       [-0.01596873, -0.07425328, -0.05047152,  0.01372938,  0.07126171,\n",
       "        -0.05240719],\n",
       "       [ 0.01098402,  0.09445286, -0.0399247 , -0.13622989, -0.05240719,\n",
       "         0.14263034]])\n",
       "      jac: array([-0.95703125,  0.31788453, -0.35074598,  0.296875  , -0.16613615,\n",
       "        0.04679678], dtype=float32)\n",
       "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
       "     nfev: 91\n",
       "      nit: 14\n",
       "     njev: 80\n",
       "   status: 2\n",
       "  success: False\n",
       "        x: array([ 5.22151741, -1.30863249, -0.25216175,  5.47852979, -5.23251959,\n",
       "       -5.20819207])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg. log like.: 20953.4609375\n",
      "neg. log like.: 20962.162109375\n",
      "neg. log like.: 20930.45703125\n",
      "neg. log like.: 20919.951171875\n",
      "neg. log like.: 20917.216796875\n",
      "neg. log like.: 20911.14453125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00359846, -0.00232967, -0.00126887],\n",
       "       [ 0.00013884, -0.00241481,  0.0022759 ],\n",
       "       [-0.00174887,  0.00284055, -0.00109165]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.scipy.optimize\n",
    "\n",
    "res = jax.scipy.optimize.minimize(\n",
    "    neglogp,\n",
    "    x0=x0,\n",
    "    method=\"BFGS\",\n",
    "    args=(\n",
    "        initial_distribution,\n",
    "        observation_log_likelihood,\n",
    "    ),\n",
    "    options={\"gtol\": 1e-2}\n",
    ")\n",
    "\n",
    "np.array(centered_softmax_forward(res.x.reshape((n_states, n_states - 1)))) - model1.transmat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizeResults(x=Array([ 5.5846224 , -0.33450836, -0.34774008,  4.910695  , -5.7176    ,\n",
       "       -4.837292  ], dtype=float32), success=Array(False, dtype=bool), status=Array(3, dtype=int32, weak_type=True), fun=Array(20911.145, dtype=float32), jac=Array([10.410156 , -4.052677 ,  2.5904493, -9.267578 , -4.9828143,\n",
       "       -8.101913 ], dtype=float32), hess_inv=Array([[ 0.09723077,  0.2433599 , -0.0471726 , -0.06384337,  0.01772271,\n",
       "        -0.04033443],\n",
       "       [ 0.24335985,  0.87410516,  0.13700712, -0.19436541,  0.03658639,\n",
       "        -0.10066385],\n",
       "       [-0.0471726 ,  0.13700719,  0.71217376,  0.3188084 , -0.25752988,\n",
       "         0.07181346],\n",
       "       [-0.06384335, -0.19436546,  0.31880838,  0.54269755, -0.01272446,\n",
       "        -0.2940241 ],\n",
       "       [ 0.01772273,  0.0365864 , -0.25752985, -0.01272443,  0.30734807,\n",
       "        -0.3491177 ],\n",
       "       [-0.04033444, -0.10066386,  0.07181344, -0.2940241 , -0.3491177 ,\n",
       "         0.64298683]], dtype=float32), nfev=Array(6, dtype=int32, weak_type=True), njev=Array(6, dtype=int32, weak_type=True), nit=Array(3, dtype=int32, weak_type=True))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(6, dtype=int32, weak_type=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.nfev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(False, dtype=bool)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(20912.73, dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marginal_log_likelihood, filtered_probs, _ = hmm_filter(\n",
    "    model1.startprob_, model1.transmat_, observation_log_likelihood\n",
    ")\n",
    "\n",
    "-1.0 * marginal_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20912.573858922045"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.hmm import forward\n",
    "\n",
    "causal_posterior, _, marginal_log_likelihood2 = forward(model1.startprob_, observation_log_likelihood, model1.transmat_)\n",
    "\n",
    "-1.0 * marginal_log_likelihood2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_hmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
